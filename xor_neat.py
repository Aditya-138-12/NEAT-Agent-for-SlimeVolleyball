import neat
import visualize


# XOR test cases: input -> expected output
xor_inputs = [(0.0, 0.0), (0.0, 1.0), (1.0, 0.0), (1.0, 1.0)]
xor_outputs = [(0.0,),    (1.0,),    (1.0,),    (0.0,)]

def eval_genomes(genomes, config):
    """Fitness function: evaluates how well each genome solves XOR."""
    for genome_id, genome in genomes:
        # Create a neural network from this genome
        net = neat.nn.FeedForwardNetwork.create(genome, config)

        # Start with perfect fitness, subtract error
        genome.fitness = 4.0

        # Test on all 4 XOR cases
        for xi, xo in zip(xor_inputs, xor_outputs):
            output = net.activate(xi)
            genome.fitness -= (output[0] - xo[0]) ** 2

# Load configuration
config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,
                     neat.DefaultSpeciesSet, neat.DefaultStagnation,
                     'config-xor')

# Create population
p = neat.Population(config)
p.add_reporter(neat.StdOutReporter(True))

stats = neat.StatisticsReporter()
p.add_reporter(stats)

# Run evolution for up to 300 generations
winner = p.run(eval_genomes, 300)

visualize.plot_stats(stats, ylog = True, view = True)
visualize.plot_species(stats, view = True)

node_names = {-1: 'A', -2: 'B', 0: 'A XOR B'}
visualize.draw_net(config, winner, view = True, node_names = node_names, prune_unused = True)

net = neat.nn.FeedForwardNetwork.create(winner, config)

inputs = [[0, 1], [1, 0], [0, 0], [1, 1]]
for i in range(len(inputs)):
	output = net.activate(inputs[i])
	if(output[0] > 0.5):
		print(1)
	else:
		print(0)

print("\n\n\n\n The output of the neural network is: \n", output)

# Test the winner
print('\nBest genome:\n{!s}'.format(winner))
print('\nBest genome ever: \n{}'.format(stats.best_genome()))
